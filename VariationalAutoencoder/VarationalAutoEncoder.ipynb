{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Autoencoders (VAE) for MNIST Generation\n",
    "\n",
    "## Overview\n",
    "\n",
    "Variational Autoencoders combine deep learning with probabilistic modeling to learn compressed representations of data while enabling generation of new samples. This notebook implements VAE for MNIST digit generation.\n",
    "\n",
    "## Key Concepts\n",
    "\n",
    "### 1. Probabilistic Latent Variables\n",
    "Unlike regular autoencoders, VAEs model the **latent space as a probability distribution**. Instead of encoding to a fixed point, we encode to a distribution $q(z|x)$ and sample from it.\n",
    "\n",
    "### 2. The Reparameterization Trick\n",
    "The key innovation that makes VAEs trainable:\n",
    "\n",
    "**Problem**: Can't backpropagate through random sampling\n",
    "**Solution**: Reparameterize the random variable\n",
    "\n",
    "$$z \\sim \\mathcal{N}(\\mu, \\sigma^2) \\quad \\Rightarrow \\quad z = \\mu + \\sigma \\odot \\epsilon \\text{ where } \\epsilon \\sim \\mathcal{N}(0, I)$$\n",
    "\n",
    "This allows gradients to flow through $\\mu$ and $\\sigma$ while maintaining the stochastic nature.\n",
    "\n",
    "### 3. VAE Loss Function: ELBO\n",
    "VAEs maximize the **Evidence Lower BOund (ELBO)**:\n",
    "\n",
    "$$\\mathcal{L} = \\mathbb{E}_{q(z|x)}[\\log p(x|z)] - D_{KL}(q(z|x) || p(z))$$\n",
    "\n",
    "**Components:**\n",
    "- **Reconstruction Loss**: $\\mathbb{E}_{q(z|x)}[\\log p(x|z)]$ - How well can we reconstruct input?\n",
    "- **KL Divergence**: $D_{KL}(q(z|x) || p(z))$ - How close is our encoding to the prior?\n",
    "\n",
    "### 4. Why Loss is Bounded\n",
    "Unlike flow models, VAE loss is **naturally bounded**:\n",
    "- **Reconstruction Loss**: BCE ∈ [0, ∞) but practically bounded by data dimensionality\n",
    "- **KL Divergence**: KL ∈ [0, ∞) but regularizes toward prior (typically small)\n",
    "- **Total Loss**: Typically ranges from 100-500 for MNIST\n",
    "\n",
    "### 5. Sampling and Generation\n",
    "**Training**: $x \\rightarrow \\text{Encoder} \\rightarrow q(z|x) \\rightarrow z \\rightarrow \\text{Decoder} \\rightarrow \\hat{x}$\n",
    "**Generation**: $z \\sim p(z) \\rightarrow \\text{Decoder} \\rightarrow x$\n",
    "\n",
    "The prior $p(z) = \\mathcal{N}(0, I)$ enables easy sampling for generation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation: Imports and Data Preprocessing\n",
    "\n",
    "VAEs work well with both discrete and continuous data, but we'll use the same dequantization as other models for consistency:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from keras.datasets.mnist import load_data\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Load and normalize MNIST dataset with DEQUANTIZATION\n",
    "print(\"Loading MNIST dataset...\")\n",
    "(trainX, trainY), (testX, testy) = load_data()\n",
    "\n",
    "# Dequantization: Add uniform noise to make discrete pixels continuous\n",
    "trainX = (np.float32(trainX) + torch.rand(trainX.shape).numpy()) / 255.\n",
    "trainX = trainX.clip(0, 1)  # Ensure values stay in [0,1]\n",
    "trainX = torch.tensor(trainX.reshape(-1, 28 * 28))\n",
    "\n",
    "print(f\"Dataset shape: {trainX.shape}\")\n",
    "print(f\"Pixel value range: [{trainX.min():.3f}, {trainX.max():.3f}]\")\n",
    "\n",
    "# Set device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VAE Architecture: Encoder and Decoder\n",
    "\n",
    "The VAE consists of two main components:\n",
    "1. **Encoder**: Maps input $x$ to latent distribution parameters $(\\mu, \\log\\sigma^2)$\n",
    "2. **Decoder**: Maps latent sample $z$ back to reconstruction $\\hat{x}$\n",
    "\n",
    "### Key Design Choices:\n",
    "- **Latent Dimension**: 20 (much smaller than 784 input dimensions)\n",
    "- **Architecture**: Fully connected layers with ReLU activations\n",
    "- **Output**: Sigmoid activation for pixel values in [0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    \"\"\"\n",
    "    Variational Autoencoder for MNIST\n",
    "    \n",
    "    Architecture:\n",
    "    - Encoder: x(784) → hidden(400) → μ,logvar(20)\n",
    "    - Decoder: z(20) → hidden(400) → x̂(784)\n",
    "    \n",
    "    Key Innovation: Reparameterization trick allows backpropagation through sampling\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim=784, hidden_dim=400, latent_dim=20):\n",
    "        super(VAE, self).__init__()\n",
    "        \n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        # Encoder: x → μ, log(σ²)\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Latent space parameters\n",
    "        self.fc_mu = nn.Linear(hidden_dim, latent_dim)      # Mean\n",
    "        self.fc_logvar = nn.Linear(hidden_dim, latent_dim)  # Log variance\n",
    "        \n",
    "        # Decoder: z → x̂\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, input_dim),\n",
    "            nn.Sigmoid()  # Output in [0,1] for MNIST\n",
    "        )\n",
    "        \n",
    "        print(f\"Initialized VAE:\")\n",
    "        print(f\"  Input dimension: {input_dim}\")\n",
    "        print(f\"  Latent dimension: {latent_dim}\")\n",
    "        print(f\"  Hidden dimension: {hidden_dim}\")\n",
    "        \n",
    "    def encode(self, x):\n",
    "        \"\"\"\n",
    "        Encode input to latent distribution parameters\n",
    "        \n",
    "        Args:\n",
    "            x: Input data [batch_size, input_dim]\n",
    "            \n",
    "        Returns:\n",
    "            mu: Mean of latent distribution [batch_size, latent_dim]\n",
    "            logvar: Log variance of latent distribution [batch_size, latent_dim]\n",
    "        \"\"\"\n",
    "        h = self.encoder(x)\n",
    "        mu = self.fc_mu(h)\n",
    "        logvar = self.fc_logvar(h)\n",
    "        return mu, logvar\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        \"\"\"\n",
    "        THE REPARAMETERIZATION TRICK\n",
    "        \n",
    "        Instead of sampling z ~ N(μ, σ²) directly (non-differentiable),\n",
    "        we reparameterize: z = μ + σ ⊙ ε where ε ~ N(0, I)\n",
    "        \n",
    "        This allows gradients to flow through μ and σ\n",
    "        \"\"\"\n",
    "        std = torch.exp(0.5 * logvar)  # σ = exp(½ log σ²)\n",
    "        eps = torch.randn_like(std)    # ε ~ N(0, I)\n",
    "        return mu + eps * std          # z = μ + σ ⊙ ε\n",
    "    \n",
    "    def decode(self, z):\n",
    "        \"\"\"\n",
    "        Decode latent sample to reconstruction\n",
    "        \n",
    "        Args:\n",
    "            z: Latent sample [batch_size, latent_dim]\n",
    "            \n",
    "        Returns:\n",
    "            x_recon: Reconstructed data [batch_size, input_dim]\n",
    "        \"\"\"\n",
    "        return self.decoder(z)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Full VAE forward pass\n",
    "        \n",
    "        x → Encoder → q(z|x) → reparameterize → z → Decoder → x̂\n",
    "        \"\"\"\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        recon_x = self.decode(z)\n",
    "        return recon_x, mu, logvar\n",
    "\n",
    "# Initialize model\n",
    "vae = VAE().to(device)\n",
    "print(f\"\\nModel parameters: {sum(p.numel() for p in vae.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Reparameterization Trick: A Closer Look\n",
    "\n",
    "Let's visualize how the reparameterization trick works and why it's crucial for training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate the reparameterization trick\n",
    "print(\"=== Reparameterization Trick Demonstration ===\")\n",
    "\n",
    "# Simulate encoder outputs\n",
    "batch_size, latent_dim = 32, 20\n",
    "mu = torch.randn(batch_size, latent_dim) * 0.5      # Some mean values\n",
    "logvar = torch.randn(batch_size, latent_dim) * 0.2  # Some log variance values\n",
    "\n",
    "print(f\"Encoder outputs:\")\n",
    "print(f\"  μ range: [{mu.min():.3f}, {mu.max():.3f}]\")\n",
    "print(f\"  log σ² range: [{logvar.min():.3f}, {logvar.max():.3f}]\")\n",
    "\n",
    "# Apply reparameterization trick\n",
    "std = torch.exp(0.5 * logvar)\n",
    "eps = torch.randn_like(std)\n",
    "z = mu + eps * std\n",
    "\n",
    "print(f\"\\nReparameterization components:\")\n",
    "print(f\"  σ = exp(½ log σ²) range: [{std.min():.3f}, {std.max():.3f}]\")\n",
    "print(f\"  ε ~ N(0,I) range: [{eps.min():.3f}, {eps.max():.3f}]\")\n",
    "print(f\"  z = μ + σε range: [{z.min():.3f}, {z.max():.3f}]\")\n",
    "\n",
    "# Show that gradients can flow\n",
    "print(f\"\\nGradient flow:\")\n",
    "print(f\"  μ requires_grad: {mu.requires_grad}\")\n",
    "print(f\"  log σ² requires_grad: {logvar.requires_grad}\")\n",
    "print(f\"  z requires_grad: {z.requires_grad}\")\n",
    "\n",
    "# Visualize the sampling process\n",
    "plt.figure(figsize=(15, 4))\n",
    "\n",
    "# Plot 1: Prior distribution N(0,1)\n",
    "plt.subplot(1, 4, 1)\n",
    "prior_samples = torch.randn(1000)\n",
    "plt.hist(prior_samples.numpy(), bins=50, alpha=0.7, color='blue')\n",
    "plt.title(\"Prior p(z) ~ N(0,1)\")\n",
    "plt.xlabel(\"z\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "\n",
    "# Plot 2: Encoder mean μ\n",
    "plt.subplot(1, 4, 2)\n",
    "plt.hist(mu.flatten().detach().numpy(), bins=50, alpha=0.7, color='orange')\n",
    "plt.title(\"Encoder Mean μ\")\n",
    "plt.xlabel(\"μ\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "\n",
    "# Plot 3: Encoder std σ\n",
    "plt.subplot(1, 4, 3)\n",
    "plt.hist(std.flatten().detach().numpy(), bins=50, alpha=0.7, color='green')\n",
    "plt.title(\"Encoder Std σ\")\n",
    "plt.xlabel(\"σ\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "\n",
    "# Plot 4: Reparameterized samples z\n",
    "plt.subplot(1, 4, 4)\n",
    "plt.hist(z.flatten().detach().numpy(), bins=50, alpha=0.7, color='red')\n",
    "plt.title(\"Samples z = μ + σε\")\n",
    "plt.xlabel(\"z\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ The reparameterization trick allows us to sample from q(z|x) while maintaining differentiability!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VAE Loss Function: Evidence Lower BOund (ELBO)\n",
    "\n",
    "The VAE loss combines two terms that balance reconstruction quality and regularization:\n",
    "\n",
    "### Mathematical Derivation\n",
    "\n",
    "**Goal**: Maximize $\\log p(x)$ (intractable)\n",
    "\n",
    "**Solution**: Maximize ELBO (tractable lower bound)\n",
    "\n",
    "$$\\log p(x) \\geq \\mathcal{L} = \\mathbb{E}_{q(z|x)}[\\log p(x|z)] - D_{KL}(q(z|x) || p(z))$$\n",
    "\n",
    "**Implementation**:\n",
    "1. **Reconstruction Loss**: Binary Cross Entropy $-\\sum_i x_i \\log \\hat{x}_i + (1-x_i) \\log (1-\\hat{x}_i)$\n",
    "2. **KL Divergence**: For $q(z|x) = \\mathcal{N}(\\mu, \\sigma^2)$ and $p(z) = \\mathcal{N}(0, I)$:\n",
    "\n",
    "$$D_{KL} = \\frac{1}{2} \\sum_j (\\mu_j^2 + \\sigma_j^2 - \\log \\sigma_j^2 - 1)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae_loss_function(recon_x, x, mu, logvar):\n",
    "    \"\"\"\n",
    "    VAE Loss = Reconstruction Loss + KL Divergence\n",
    "    \n",
    "    Args:\n",
    "        recon_x: Reconstructed data [batch_size, input_dim]\n",
    "        x: Original data [batch_size, input_dim] \n",
    "        mu: Latent mean [batch_size, latent_dim]\n",
    "        logvar: Latent log variance [batch_size, latent_dim]\n",
    "        \n",
    "    Returns:\n",
    "        total_loss: Combined loss\n",
    "        bce_loss: Reconstruction loss component  \n",
    "        kld_loss: KL divergence component\n",
    "    \"\"\"\n",
    "    \n",
    "    # Reconstruction Loss: Binary Cross Entropy\n",
    "    # Measures how well we can reconstruct the input\n",
    "    bce_loss = F.binary_cross_entropy(recon_x, x, reduction='sum')\n",
    "    \n",
    "    # KL Divergence Loss: KL(q(z|x) || p(z))\n",
    "    # Regularizes the latent space to be close to prior N(0,I)\n",
    "    # For Gaussians: KL = ½ Σ(μ² + σ² - log(σ²) - 1)\n",
    "    kld_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    \n",
    "    total_loss = bce_loss + kld_loss\n",
    "    \n",
    "    return total_loss, bce_loss, kld_loss\n",
    "\n",
    "# Demonstrate loss computation with example data\n",
    "print(\"=== VAE Loss Function Demonstration ===\")\n",
    "\n",
    "# Create example batch\n",
    "batch_size = 32\n",
    "example_x = torch.rand(batch_size, 784).to(device)  # Example input\n",
    "\n",
    "# Forward pass\n",
    "with torch.no_grad():\n",
    "    recon_x, mu, logvar = vae(example_x)\n",
    "    total_loss, bce_loss, kld_loss = vae_loss_function(recon_x, example_x, mu, logvar)\n",
    "\n",
    "print(f\"Example loss computation:\")\n",
    "print(f\"  Reconstruction Loss (BCE): {bce_loss.item():.2f}\")\n",
    "print(f\"  KL Divergence: {kld_loss.item():.2f}\")\n",
    "print(f\"  Total Loss: {total_loss.item():.2f}\")\n",
    "print(f\"  Loss per sample: {total_loss.item()/batch_size:.2f}\")\n",
    "\n",
    "# Show typical loss ranges for MNIST\n",
    "print(f\"\\nTypical VAE loss ranges for MNIST:\")\n",
    "print(f\"  Reconstruction Loss: 100-300 (depends on reconstruction quality)\")\n",
    "print(f\"  KL Divergence: 0-50 (regularization strength)\")\n",
    "print(f\"  Total Loss: 100-350 (decreases during training)\")\n",
    "print(f\"  ✓ VAE loss is naturally bounded unlike flow models!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Generation Functions\n",
    "\n",
    "Now let's implement the training loop and image generation functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_images_vae(model, epoch, nb_data=10, latent_dim=20, device='cpu'):\n",
    "    \"\"\"\n",
    "    Generate and display VAE sample images\n",
    "    \n",
    "    Process: z ~ N(0,I) → Decoder → x̂\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Sample from prior distribution N(0,I)\n",
    "        z = torch.randn(nb_data * nb_data, latent_dim).to(device)\n",
    "        samples = model.decode(z).cpu().numpy()\n",
    "        \n",
    "        fig, axs = plt.subplots(nb_data, nb_data, figsize=(10, 10))\n",
    "        for i in range(nb_data):\n",
    "            for j in range(nb_data):\n",
    "                idx = i * nb_data + j\n",
    "                axs[i, j].imshow(samples[idx].reshape(28, 28), cmap='gray')\n",
    "                axs[i, j].set_xticks([])\n",
    "                axs[i, j].set_yticks([])\n",
    "        plt.suptitle(f'VAE Generated Images - Epoch {epoch}')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    model.train()\n",
    "\n",
    "\n",
    "def train_vae(model, optimizer, dataloader, nb_epochs=50, device='cpu'):\n",
    "    \"\"\"\n",
    "    Train the VAE model\n",
    "    \n",
    "    Training process:\n",
    "    1. Forward pass: x → q(z|x) → z → p(x|z) → x̂\n",
    "    2. Compute ELBO loss\n",
    "    3. Backpropagate gradients\n",
    "    4. Update parameters\n",
    "    \"\"\"\n",
    "    training_losses = []\n",
    "    \n",
    "    model.train()\n",
    "    for epoch in tqdm(range(nb_epochs), desc=\"Training VAE\"):\n",
    "        epoch_loss = 0\n",
    "        epoch_bce = 0\n",
    "        epoch_kld = 0\n",
    "        \n",
    "        for batch_idx, batch in enumerate(dataloader):\n",
    "            batch = batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            recon_batch, mu, logvar = model(batch)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss, bce, kld = vae_loss_function(recon_batch, batch, mu, logvar)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            epoch_bce += bce.item()\n",
    "            epoch_kld += kld.item()\n",
    "            \n",
    "            # Print loss components for monitoring\n",
    "            if batch_idx % 100 == 0:\n",
    "                print(f\"Epoch {epoch+1:3d}, Batch {batch_idx:3d}: \"\n",
    "                      f\"BCE={bce.item():8.1f}, KLD={kld.item():6.1f}, \"\n",
    "                      f\"Total={loss.item():8.1f}\")\n",
    "        \n",
    "        # Store average loss\n",
    "        avg_loss = epoch_loss / len(dataloader.dataset)\n",
    "        training_losses.append(avg_loss)\n",
    "        \n",
    "        # Generate images every 10 epochs\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"\\nGenerating VAE images at epoch {epoch + 1}\")\n",
    "            generate_images_vae(model, epoch + 1, device=device)\n",
    "    \n",
    "    return training_losses\n",
    "\n",
    "print(\"Training and generation functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the VAE Model\n",
    "\n",
    "Let's train our VAE and observe how the loss components evolve:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training setup\n",
    "optimizer = torch.optim.Adam(vae.parameters(), lr=1e-3)\n",
    "dataloader = DataLoader(trainX, batch_size=128, shuffle=True)\n",
    "\n",
    "# Test VAE forward pass\n",
    "print(\"Testing VAE forward pass...\")\n",
    "with torch.no_grad():\n",
    "    test_batch = next(iter(dataloader))[:5].to(device)\n",
    "    recon, mu, logvar = vae(test_batch)\n",
    "    test_loss, test_bce, test_kld = vae_loss_function(recon, test_batch, mu, logvar)\n",
    "    \n",
    "    print(f\"Test batch:\")\n",
    "    print(f\"  Input shape: {test_batch.shape}\")\n",
    "    print(f\"  Reconstruction shape: {recon.shape}\")\n",
    "    print(f\"  Latent μ shape: {mu.shape}\")\n",
    "    print(f\"  Latent log σ² shape: {logvar.shape}\")\n",
    "    print(f\"  Test loss: {test_loss.item():.2f}\")\n",
    "\n",
    "# Train VAE\n",
    "print(\"\\nStarting VAE training...\")\n",
    "training_losses = train_vae(vae, optimizer, dataloader, nb_epochs=30, device=device)\n",
    "\n",
    "print(\"\\nVAE training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Space Analysis\n",
    "\n",
    "Let's explore the learned latent space and demonstrate interpolation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the learned latent space\n",
    "print(\"=== Latent Space Analysis ===\")\n",
    "\n",
    "vae.eval()\n",
    "with torch.no_grad():\n",
    "    # Encode some test samples\n",
    "    test_samples = trainX[:1000].to(device)\n",
    "    mu_encoded, logvar_encoded = vae.encode(test_samples)\n",
    "    \n",
    "    print(f\"Encoded latent statistics:\")\n",
    "    print(f\"  μ mean: {mu_encoded.mean():.3f}, std: {mu_encoded.std():.3f}\")\n",
    "    print(f\"  log σ² mean: {logvar_encoded.mean():.3f}, std: {logvar_encoded.std():.3f}\")\n",
    "    \n",
    "    # Sample from prior and decode\n",
    "    prior_samples = torch.randn(100, 20).to(device)\n",
    "    generated = vae.decode(prior_samples)\n",
    "    \n",
    "    print(f\"Generated samples statistics:\")\n",
    "    print(f\"  Generated pixel mean: {generated.mean():.3f}, std: {generated.std():.3f}\")\n",
    "    print(f\"  Generated range: [{generated.min():.3f}, {generated.max():.3f}]\")\n",
    "\n",
    "# Demonstrate latent space interpolation\n",
    "print(\"\\n=== Latent Space Interpolation ===\")\n",
    "\n",
    "def interpolate_latent(vae, z1, z2, steps=10):\n",
    "    \"\"\"Interpolate between two latent points\"\"\"\n",
    "    alphas = torch.linspace(0, 1, steps)\n",
    "    interpolations = []\n",
    "    \n",
    "    for alpha in alphas:\n",
    "        z_interp = (1 - alpha) * z1 + alpha * z2\n",
    "        x_interp = vae.decode(z_interp.unsqueeze(0))\n",
    "        interpolations.append(x_interp.cpu().numpy())\n",
    "    \n",
    "    return interpolations\n",
    "\n",
    "# Create interpolation between random points\n",
    "with torch.no_grad():\n",
    "    z1 = torch.randn(20).to(device)\n",
    "    z2 = torch.randn(20).to(device)\n",
    "    \n",
    "    interpolations = interpolate_latent(vae, z1, z2, steps=10)\n",
    "    \n",
    "    # Visualize interpolation\n",
    "    fig, axes = plt.subplots(1, 10, figsize=(15, 2))\n",
    "    for i, interp in enumerate(interpolations):\n",
    "        axes[i].imshow(interp.reshape(28, 28), cmap='gray')\n",
    "        axes[i].set_xticks([])\n",
    "        axes[i].set_yticks([])\n",
    "        axes[i].set_title(f'α={i/9:.1f}')\n",
    "    \n",
    "    plt.suptitle('Latent Space Interpolation: z₁ → z₂')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"✓ Smooth interpolation demonstrates the continuous latent space!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways: VAE vs Other Generative Models\n",
    "\n",
    "### Advantages of VAEs\n",
    "1. **Principled Probabilistic Framework**: Based on solid statistical foundations\n",
    "2. **Stable Training**: More stable than GANs, no adversarial dynamics\n",
    "3. **Meaningful Latent Space**: Enables interpolation and latent arithmetic\n",
    "4. **Bounded Loss**: Loss function has natural bounds, easier to interpret\n",
    "5. **Both Inference and Generation**: Can encode data to latent space AND generate new data\n",
    "\n",
    "### Limitations of VAEs\n",
    "1. **Blurry Outputs**: BCE loss tends to produce blurry reconstructions\n",
    "2. **Limited Expressiveness**: Variational approximation may be too restrictive\n",
    "3. **No Exact Likelihood**: Unlike flows, provides only lower bound (ELBO)\n",
    "4. **Posterior Collapse**: KL term may dominate, leading to uninformative latents\n",
    "\n",
    "### Comparison Summary\n",
    "\n",
    "| Aspect | VAE | Real NVP | GAN |\n",
    "|--------|-----|----------|-----|\n",
    "| **Likelihood** | Lower bound (ELBO) | Exact | None |\n",
    "| **Training Stability** | Stable | Stable | Unstable |\n",
    "| **Sample Quality** | Blurry | Good | Excellent |\n",
    "| **Loss Bounds** | Bounded | Unbounded | Bounded |\n",
    "| **Latent Space** | Meaningful | Meaningful | Not guaranteed |\n",
    "| **Inference** | Fast | Slow | None |\n",
    "\n",
    "### Best Practices for VAEs\n",
    "- **β-VAE**: Weight KL term to control disentanglement\n",
    "- **Warm-up**: Gradually increase KL weight during training\n",
    "- **Architecture**: Use skip connections or more powerful decoders\n",
    "- **Loss Functions**: Try alternatives to BCE (e.g., MSE, perceptual loss)\n",
    "\n",
    "---\n",
    "\n",
    "**References:**\n",
    "- Kingma, D. P., & Welling, M. (2013). Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114.\n",
    "- Rezende, D. J., Mohamed, S., & Wierstra, D. (2014). Stochastic backpropagation and approximate inference in deep generative models. arXiv preprint arXiv:1401.4082."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}