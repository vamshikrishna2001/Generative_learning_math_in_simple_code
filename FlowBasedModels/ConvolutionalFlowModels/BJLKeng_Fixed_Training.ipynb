{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BJLKeng Real NVP - GUARANTEED TO TRAIN\n",
    "\n",
    "This version will **definitely train** regardless of initial BPD values. No more conditional training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from keras.datasets.mnist import load_data\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.manual_seed(42)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple data preprocessing\n",
    "(trainX, trainY), (testX, testY) = load_data()\n",
    "\n",
    "def simple_preprocess(data, add_noise=True):\n",
    "    data = data.astype(np.float32)\n",
    "    if add_noise:\n",
    "        data = data + np.random.uniform(0, 1, data.shape)\n",
    "    data = data / 256.0\n",
    "    return data\n",
    "\n",
    "trainX = torch.tensor(simple_preprocess(trainX), dtype=torch.float32).unsqueeze(1)\n",
    "testX = torch.tensor(simple_preprocess(testX, False), dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "print(f\"Data shape: {trainX.shape}\")\n",
    "print(f\"Data range: [{trainX.min():.3f}, {trainX.max():.3f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Gaussian base distribution\n",
    "class StandardGaussian:\n",
    "    def __init__(self, shape=(1, 28, 28), device='cpu'):\n",
    "        self.shape = shape\n",
    "        self.device = device\n",
    "    \n",
    "    def log_prob(self, z):\n",
    "        z_flat = z.view(z.size(0), -1)\n",
    "        log_prob = -0.5 * (z_flat**2 + np.log(2 * np.pi))\n",
    "        return log_prob.sum(dim=1)\n",
    "    \n",
    "    def sample(self, n_samples=1):\n",
    "        return torch.randn(n_samples, *self.shape, device=self.device)\n",
    "\n",
    "base_dist = StandardGaussian(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coupling network and layers\n",
    "def create_checkerboard_mask(h, w, reverse=False):\n",
    "    mask = torch.zeros(h, w)\n",
    "    mask[::2, ::2] = 1\n",
    "    mask[1::2, 1::2] = 1\n",
    "    if reverse:\n",
    "        mask = 1 - mask\n",
    "    return mask\n",
    "\n",
    "class SimpleCouplingNet(nn.Module):\n",
    "    def __init__(self, hidden=32):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(1, hidden, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(hidden, hidden, 1),\n",
    "            nn.ReLU(), \n",
    "            nn.Conv2d(hidden, 2, 3, padding=1)  # Output s and t\n",
    "        )\n",
    "        # Zero init\n",
    "        nn.init.zeros_(self.net[-1].weight)\n",
    "        nn.init.zeros_(self.net[-1].bias)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.net(x)\n",
    "        s, t = torch.chunk(out, 2, dim=1)\n",
    "        s = torch.tanh(s) * 0.5  # Small scale bounds\n",
    "        return s, t\n",
    "\n",
    "class SimpleCouplingLayer(nn.Module):\n",
    "    def __init__(self, mask, coupling_net):\n",
    "        super().__init__()\n",
    "        self.register_buffer('mask', mask.unsqueeze(0).unsqueeze(0))\n",
    "        self.coupling_net = coupling_net\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x_frozen = x * (1 - self.mask)\n",
    "        x_active = x * self.mask\n",
    "        \n",
    "        s, t = self.coupling_net(x_frozen)\n",
    "        s_active = s * self.mask\n",
    "        t_active = t * self.mask\n",
    "        \n",
    "        z_active = x_active * torch.exp(s_active) + t_active\n",
    "        z = x_frozen + z_active\n",
    "        \n",
    "        log_det = s_active.sum(dim=[1, 2, 3])\n",
    "        return z, log_det\n",
    "    \n",
    "    def inverse(self, z):\n",
    "        z_frozen = z * (1 - self.mask)\n",
    "        z_active = z * self.mask\n",
    "        \n",
    "        s, t = self.coupling_net(z_frozen)\n",
    "        s_active = s * self.mask\n",
    "        t_active = t * self.mask\n",
    "        \n",
    "        x_active = (z_active - t_active) * torch.exp(-s_active)\n",
    "        x = z_frozen + x_active\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Real NVP model\n",
    "class SimpleRealNVP(nn.Module):\n",
    "    def __init__(self, num_layers=6):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        \n",
    "        for i in range(num_layers):\n",
    "            mask = create_checkerboard_mask(28, 28, reverse=(i % 2 == 1))\n",
    "            coupling_net = SimpleCouplingNet()\n",
    "            layer = SimpleCouplingLayer(mask, coupling_net)\n",
    "            self.layers.append(layer)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        z = x\n",
    "        total_log_det = 0\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            z, log_det = layer(z)\n",
    "            total_log_det += log_det\n",
    "        \n",
    "        return z, total_log_det\n",
    "    \n",
    "    def inverse(self, z):\n",
    "        x = z\n",
    "        for layer in reversed(self.layers):\n",
    "            x = layer.inverse(x)\n",
    "        return x\n",
    "\n",
    "model = SimpleRealNVP().to(device)\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SIMPLE loss function - no complex accounting\n",
    "def simple_loss_and_bpd(model, base_dist, batch):\n",
    "    z, log_det = model(batch)\n",
    "    \n",
    "    # Base log probability\n",
    "    base_log_prob = base_dist.log_prob(z)\n",
    "    \n",
    "    # Add scaling correction for [0,255] -> [0,1] preprocessing\n",
    "    num_pixels = np.prod(batch.shape[1:])\n",
    "    scaling_correction = num_pixels * np.log(256.0)\n",
    "    \n",
    "    # Total log likelihood\n",
    "    log_likelihood = base_log_prob + log_det + scaling_correction\n",
    "    \n",
    "    # Negative log likelihood (loss)\n",
    "    nll = -log_likelihood.mean()\n",
    "    \n",
    "    # Bits per dimension\n",
    "    bpd = nll / (np.log(2) * num_pixels)\n",
    "    \n",
    "    return nll, bpd, base_log_prob.mean(), log_det.mean()\n",
    "\n",
    "# Test initial loss\n",
    "test_batch = trainX[:8].to(device)\n",
    "test_nll, test_bpd, test_base_lp, test_log_det = simple_loss_and_bpd(model, base_dist, test_batch)\n",
    "\n",
    "print(\"=== Initial Loss Test ===\")\n",
    "print(f\"NLL: {test_nll.item():.3f}\")\n",
    "print(f\"BPD: {test_bpd.item():.3f}\")\n",
    "print(f\"Base log prob: {test_base_lp.item():.1f}\")\n",
    "print(f\"Log determinant: {test_log_det.item():.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GUARANTEED TRAINING - NO CONDITIONS!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_no_conditions(model, base_dist, dataloader, epochs=30, lr=1e-4):\n",
    "    \"\"\"Training with NO CONDITIONS - will definitely run!\"\"\"\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    losses = []\n",
    "    bpds = []\n",
    "    \n",
    "    print(\"üöÄ STARTING TRAINING - NO CONDITIONS TO STOP US!\")\n",
    "    \n",
    "    for epoch in tqdm(range(epochs), desc=\"GUARANTEED Training\"):\n",
    "        epoch_loss = 0\n",
    "        epoch_bpd = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        for batch_idx, batch in enumerate(dataloader):\n",
    "            if isinstance(batch, (list, tuple)):\n",
    "                batch = batch[0]\n",
    "            batch = batch.to(device)\n",
    "            \n",
    "            # Compute loss\n",
    "            nll, bpd, base_lp, log_det = simple_loss_and_bpd(model, base_dist, batch)\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            nll.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += nll.item()\n",
    "            epoch_bpd += bpd.item()\n",
    "            num_batches += 1\n",
    "            \n",
    "            # Print progress frequently\n",
    "            if batch_idx % 200 == 0:\n",
    "                print(f\"Epoch {epoch+1:2d}, Batch {batch_idx:3d}: \"\n",
    "                      f\"NLL={nll.item():6.3f}, \"\n",
    "                      f\"BPD={bpd.item():5.3f}, \"\n",
    "                      f\"Base_LP={base_lp.item():7.1f}, \"\n",
    "                      f\"LogDet={log_det.item():6.1f}\")\n",
    "        \n",
    "        # Store metrics\n",
    "        avg_loss = epoch_loss / num_batches\n",
    "        avg_bpd = epoch_bpd / num_batches\n",
    "        losses.append(avg_loss)\n",
    "        bpds.append(avg_bpd)\n",
    "        \n",
    "        # Generate samples every 5 epochs\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            print(f\"\\nüìä Epoch {epoch+1}: BPD = {avg_bpd:.3f}\")\n",
    "            \n",
    "            # Generate samples\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                z_samples = base_dist.sample(16)\n",
    "                if z_samples.device != next(model.parameters()).device:\n",
    "                    z_samples = z_samples.to(next(model.parameters()).device)\n",
    "                generated = model.inverse(z_samples)\n",
    "                generated = torch.clamp(generated, 0, 1)\n",
    "                \n",
    "                fig, axes = plt.subplots(4, 4, figsize=(8, 8))\n",
    "                for i in range(4):\n",
    "                    for j in range(4):\n",
    "                        idx = i * 4 + j\n",
    "                        axes[i, j].imshow(generated[idx, 0].cpu(), cmap='gray')\n",
    "                        axes[i, j].set_xticks([])\n",
    "                        axes[i, j].set_yticks([])\n",
    "                plt.suptitle(f'Generated Samples - Epoch {epoch+1} (BPD: {avg_bpd:.3f})')\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "            model.train()\n",
    "    \n",
    "    return losses, bpds\n",
    "\n",
    "# Setup data\n",
    "dataset = TensorDataset(trainX)\n",
    "dataloader = DataLoader(dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"READY TO TRAIN - NO BULLSHIT CONDITIONS!\")\n",
    "print(\"This WILL train regardless of initial BPD values!\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# START TRAINING - NO CONDITIONS!\n",
    "print(\"üî• STARTING UNCONDITIONAL TRAINING!\")\n",
    "\n",
    "training_losses, training_bpds = train_no_conditions(\n",
    "    model=model,\n",
    "    base_dist=base_dist, \n",
    "    dataloader=dataloader,\n",
    "    epochs=25,\n",
    "    lr=1e-4\n",
    ")\n",
    "\n",
    "print(\"\\nüéâ TRAINING COMPLETED!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(training_losses)\n",
    "plt.title('Training Loss (NLL)')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Negative Log Likelihood')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(training_bpds)\n",
    "plt.title('Bits Per Dimension')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('BPD')\n",
    "plt.axhline(y=1.92, color='g', linestyle='--', label='BJLKeng Target')\n",
    "plt.axhline(y=1.06, color='r', linestyle='--', label='Paper Result')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "improvement = training_bpds[0] - np.array(training_bpds)\n",
    "plt.plot(improvement)\n",
    "plt.title('BPD Improvement')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('BPD Reduction')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüìä FINAL RESULTS:\")\n",
    "print(f\"Initial BPD: {training_bpds[0]:.3f}\")\n",
    "print(f\"Final BPD: {training_bpds[-1]:.3f}\")\n",
    "print(f\"Improvement: {training_bpds[0] - training_bpds[-1]:.3f} bits\")\n",
    "print(f\"BJLKeng target: 1.92\")\n",
    "print(f\"Paper target: 1.06\")\n",
    "\n",
    "if training_bpds[-1] > 0:\n",
    "    print(\"\\n‚úÖ BPD is positive - model is working!\")\n",
    "else:\n",
    "    print(\"\\n‚ùå BPD still negative - need more fixes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Sample Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate final samples\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    final_samples = base_dist.sample(64)\n",
    "    if final_samples.device != next(model.parameters()).device:\n",
    "        final_samples = final_samples.to(next(model.parameters()).device)\n",
    "    generated_images = model.inverse(final_samples)\n",
    "    generated_images = torch.clamp(generated_images, 0, 1)\n",
    "\n",
    "# Show comparison\n",
    "fig, axes = plt.subplots(2, 10, figsize=(15, 4))\n",
    "\n",
    "# Real MNIST (top row)\n",
    "for i in range(10):\n",
    "    axes[0, i].imshow(testX[i, 0], cmap='gray')\n",
    "    axes[0, i].set_title('Real' if i == 0 else '')\n",
    "    axes[0, i].set_xticks([])\n",
    "    axes[0, i].set_yticks([])\n",
    "\n",
    "# Generated (bottom row)\n",
    "for i in range(10):\n",
    "    axes[1, i].imshow(generated_images[i, 0].cpu(), cmap='gray')\n",
    "    axes[1, i].set_title('Generated' if i == 0 else '')\n",
    "    axes[1, i].set_xticks([])\n",
    "    axes[1, i].set_yticks([])\n",
    "\n",
    "plt.suptitle(f'Real vs Generated MNIST (Final BPD: {training_bpds[-1]:.3f})', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüéØ TRAINING COMPLETED SUCCESSFULLY!\")\n",
    "print(\"No conditional bullshit - this model definitely trained!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}