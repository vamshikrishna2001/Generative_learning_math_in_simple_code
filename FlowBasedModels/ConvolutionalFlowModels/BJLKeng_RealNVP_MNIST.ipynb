{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BJLKeng-Style Real NVP for MNIST\n",
    "\n",
    "Based on the working implementation from BJLKeng's blog post and GitHub repo. This version follows their exact approach that achieved **1.92 bits/dim** on MNIST.\n",
    "\n",
    "## Key Principles from BJLKeng:\n",
    "1. **Simple preprocessing**: Just scale MNIST to [0,1] - no complex transforms\n",
    "2. **Batch norm in coupling layers**: With proper loss accounting\n",
    "3. **Small learning rate**: 0.0005 (\"slow learners\")\n",
    "4. **Zero initialization**: Start networks at identity\n",
    "5. **Long training**: Many epochs needed\n",
    "\n",
    "**Target**: ~1.9 bits/dim (BJLKeng's result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from keras.datasets.mnist import load_data\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.manual_seed(42)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BJLKeng's Simple Data Preprocessing\n",
    "\n",
    "**Quote**: *\"Eschewed the pixel transform for MNIST because it's not really a natural image. Scaling pixel values to [0, 1] seemed to work better.\"*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST - BJLKeng's simple approach\n",
    "(trainX, trainY), (testX, testY) = load_data()\n",
    "\n",
    "# Simple preprocessing like BJLKeng\n",
    "def bjlkeng_preprocess(data, add_noise=True):\n",
    "    \"\"\"BJLKeng's preprocessing: simple scaling to [0,1]\"\"\"\n",
    "    data = data.astype(np.float32)\n",
    "    \n",
    "    if add_noise:\n",
    "        # Add uniform noise for dequantization\n",
    "        data = data + np.random.uniform(0, 1, data.shape)\n",
    "    \n",
    "    # Scale to [0, 1] - BJLKeng's approach\n",
    "    data = data / 256.0\n",
    "    \n",
    "    return data\n",
    "\n",
    "trainX_processed = bjlkeng_preprocess(trainX, add_noise=True)\n",
    "testX_processed = bjlkeng_preprocess(testX, add_noise=False)\n",
    "\n",
    "# Convert to tensors\n",
    "trainX = torch.tensor(trainX_processed, dtype=torch.float32).unsqueeze(1)\n",
    "testX = torch.tensor(testX_processed, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "print(f\"Training data shape: {trainX.shape}\")\n",
    "print(f\"Data range: [{trainX.min():.3f}, {trainX.max():.3f}]\")\n",
    "print(f\"Data mean: {trainX.mean():.3f}\")\n",
    "print(f\"Data std: {trainX.std():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard Normal Base Distribution\n",
    "\n",
    "Following BJLKeng's approach with standard Gaussian prior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StandardGaussian:\n",
    "    \"\"\"Standard Gaussian base distribution\"\"\"\n",
    "    \n",
    "    def __init__(self, shape=(1, 28, 28), device='cpu'):\n",
    "        self.shape = shape\n",
    "        self.device = device\n",
    "        self.dim = np.prod(shape)\n",
    "    \n",
    "    def log_prob(self, z):\n",
    "        \"\"\"Log probability of standard Gaussian\"\"\"\n",
    "        z_flat = z.view(z.size(0), -1)\n",
    "        # Standard Gaussian: log p(z) = -0.5 * (z^2 + log(2Ï€))\n",
    "        log_prob = -0.5 * (z_flat**2 + np.log(2 * np.pi))\n",
    "        return log_prob.sum(dim=1)\n",
    "    \n",
    "    def sample(self, n_samples=1):\n",
    "        \"\"\"Sample from standard Gaussian\"\"\"\n",
    "        return torch.randn(n_samples, *self.shape, device=self.device)\n",
    "\n",
    "base_dist = StandardGaussian(device=device)\n",
    "\n",
    "# Test base distribution\n",
    "test_z = base_dist.sample(5)\n",
    "test_logp = base_dist.log_prob(test_z)\n",
    "print(f\"Base sample shape: {test_z.shape}\")\n",
    "print(f\"Base log prob: {test_logp.mean():.1f} (expected: ~{-0.5 * 784 * (1 + np.log(2*np.pi)):.1f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BJLKeng's Coupling Network with Batch Norm\n",
    "\n",
    "Key features:\n",
    "- **Batch normalization** in coupling networks\n",
    "- **Zero initialization** for identity start\n",
    "- **Running average batch norm** with modified momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_checkerboard_mask(h, w, reverse=False):\n",
    "    \"\"\"Checkerboard mask\"\"\"\n",
    "    mask = torch.zeros(h, w)\n",
    "    mask[::2, ::2] = 1\n",
    "    mask[1::2, 1::2] = 1\n",
    "    if reverse:\n",
    "        mask = 1 - mask\n",
    "    return mask\n",
    "\n",
    "\n",
    "class BJLKengCouplingNet(nn.Module):\n",
    "    \"\"\"BJLKeng's coupling network with batch norm\"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels=1, hidden_channels=64, num_layers=3):\n",
    "        super().__init__()\n",
    "        \n",
    "        layers = []\n",
    "        \n",
    "        # Input layer\n",
    "        layers.append(nn.Conv2d(in_channels, hidden_channels, 3, padding=1))\n",
    "        layers.append(nn.BatchNorm2d(hidden_channels, momentum=0.05))  # BJLKeng uses small momentum\n",
    "        layers.append(nn.ReLU())\n",
    "        \n",
    "        # Hidden layers\n",
    "        for _ in range(num_layers - 2):\n",
    "            layers.append(nn.Conv2d(hidden_channels, hidden_channels, 3, padding=1))\n",
    "            layers.append(nn.BatchNorm2d(hidden_channels, momentum=0.05))\n",
    "            layers.append(nn.ReLU())\n",
    "        \n",
    "        # Output layer - outputs both s and t\n",
    "        layers.append(nn.Conv2d(hidden_channels, 2 * in_channels, 3, padding=1))\n",
    "        \n",
    "        self.network = nn.Sequential(*layers)\n",
    "        \n",
    "        # BJLKeng: Initialize output layer to zero for identity start\n",
    "        nn.init.zeros_(self.network[-1].weight)\n",
    "        nn.init.zeros_(self.network[-1].bias)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.network(x)\n",
    "        s, t = torch.chunk(out, 2, dim=1)\n",
    "        \n",
    "        # BJLKeng: tanh scaling for s\n",
    "        s = torch.tanh(s)  # [-1, 1] range\n",
    "        \n",
    "        return s, t\n",
    "\n",
    "\n",
    "class BJLKengCouplingLayer(nn.Module):\n",
    "    \"\"\"BJLKeng's coupling layer implementation\"\"\"\n",
    "    \n",
    "    def __init__(self, mask, coupling_net):\n",
    "        super().__init__()\n",
    "        self.register_buffer('mask', mask.unsqueeze(0).unsqueeze(0))  # [1, 1, H, W]\n",
    "        self.coupling_net = coupling_net\n",
    "        \n",
    "    def forward(self, x, compute_log_det=True):\n",
    "        \"\"\"Forward pass with optional log determinant\"\"\"\n",
    "        # Split according to mask\n",
    "        x_frozen = x * (1 - self.mask)\n",
    "        x_active = x * self.mask\n",
    "        \n",
    "        # Get s and t from frozen part\n",
    "        s, t = self.coupling_net(x_frozen)\n",
    "        \n",
    "        # Apply transformation only to active part\n",
    "        s_active = s * self.mask\n",
    "        t_active = t * self.mask\n",
    "        \n",
    "        # Affine transformation\n",
    "        z_active = x_active * torch.exp(s_active) + t_active\n",
    "        z = x_frozen + z_active\n",
    "        \n",
    "        if compute_log_det:\n",
    "            # Log determinant\n",
    "            log_det = s_active.sum(dim=[1, 2, 3])\n",
    "            return z, log_det\n",
    "        else:\n",
    "            return z\n",
    "    \n",
    "    def inverse(self, z):\n",
    "        \"\"\"Inverse transformation\"\"\"\n",
    "        z_frozen = z * (1 - self.mask)\n",
    "        z_active = z * self.mask\n",
    "        \n",
    "        s, t = self.coupling_net(z_frozen)\n",
    "        s_active = s * self.mask\n",
    "        t_active = t * self.mask\n",
    "        \n",
    "        # Inverse affine transformation\n",
    "        x_active = (z_active - t_active) * torch.exp(-s_active)\n",
    "        x = z_frozen + x_active\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Test coupling layer\n",
    "test_mask = create_checkerboard_mask(28, 28)\n",
    "test_coupling_net = BJLKengCouplingNet()\n",
    "test_layer = BJLKengCouplingLayer(test_mask, test_coupling_net)\n",
    "\n",
    "test_x = torch.randn(2, 1, 28, 28) * 0.1\n",
    "test_z, test_logdet = test_layer(test_x)\n",
    "test_x_recon = test_layer.inverse(test_z)\n",
    "\n",
    "print(f\"Coupling layer test:\")\n",
    "print(f\"  Input shape: {test_x.shape}\")\n",
    "print(f\"  Output shape: {test_z.shape}\")\n",
    "print(f\"  Reconstruction error: {torch.max(torch.abs(test_x - test_x_recon)):.8f}\")\n",
    "print(f\"  Log determinant: {test_logdet}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BJLKeng's Real NVP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BJLKengRealNVP(nn.Module):\n",
    "    \"\"\"BJLKeng's Real NVP implementation\"\"\"\n",
    "    \n",
    "    def __init__(self, num_coupling_layers=8, hidden_channels=64):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_coupling_layers = num_coupling_layers\n",
    "        self.coupling_layers = nn.ModuleList()\n",
    "        \n",
    "        # Create coupling layers with alternating masks\n",
    "        for i in range(num_coupling_layers):\n",
    "            mask = create_checkerboard_mask(28, 28, reverse=(i % 2 == 1))\n",
    "            coupling_net = BJLKengCouplingNet(hidden_channels=hidden_channels)\n",
    "            coupling_layer = BJLKengCouplingLayer(mask, coupling_net)\n",
    "            self.coupling_layers.append(coupling_layer)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass through all coupling layers\"\"\"\n",
    "        z = x\n",
    "        total_log_det = 0\n",
    "        \n",
    "        for layer in self.coupling_layers:\n",
    "            z, log_det = layer(z)\n",
    "            total_log_det += log_det\n",
    "        \n",
    "        return z, total_log_det\n",
    "    \n",
    "    def inverse(self, z):\n",
    "        \"\"\"Inverse pass (generation)\"\"\"\n",
    "        x = z\n",
    "        for layer in reversed(self.coupling_layers):\n",
    "            x = layer.inverse(x)\n",
    "        return x\n",
    "    \n",
    "    def sample(self, base_dist, n_samples=64):\n",
    "        \"\"\"Generate samples\"\"\"\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            z = base_dist.sample(n_samples)\n",
    "            if z.device != next(self.parameters()).device:\n",
    "                z = z.to(next(self.parameters()).device)\n",
    "            x = self.inverse(z)\n",
    "        self.train()\n",
    "        return x\n",
    "\n",
    "# Create model\n",
    "model = BJLKengRealNVP(num_coupling_layers=8, hidden_channels=64).to(device)\n",
    "\n",
    "# Test model\n",
    "test_batch = trainX[:4].to(device)\n",
    "test_z, test_logdet = model(test_batch)\n",
    "test_recon = model.inverse(test_z)\n",
    "\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"Forward test: {test_batch.shape} -> {test_z.shape}\")\n",
    "print(f\"Reconstruction error: {torch.max(torch.abs(test_batch - test_recon)):.8f}\")\n",
    "print(f\"Log determinant range: [{test_logdet.min():.2f}, {test_logdet.max():.2f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BJLKeng's Loss Function with Batch Norm Accounting\n",
    "\n",
    "Key insight: BJLKeng accounts for batch normalization in the loss function since it's also a transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bjlkeng_loss_and_bpd(model, base_dist, batch, l2_reg=1e-5):\n",
    "    \"\"\"BJLKeng's loss computation with batch norm accounting\"\"\"\n",
    "    \n",
    "    # Forward pass\n",
    "    z, coupling_log_det = model(batch)\n",
    "    \n",
    "    # Base distribution log probability\n",
    "    base_log_prob = base_dist.log_prob(z)\n",
    "    \n",
    "    # Account for batch norm scaling (BJLKeng insight)\n",
    "    # Batch norm also transforms the density\n",
    "    bn_log_det = 0\n",
    "    for module in model.modules():\n",
    "        if isinstance(module, nn.BatchNorm2d):\n",
    "            # Batch norm log determinant: log|det(scale/std)|\n",
    "            bn_log_det += torch.sum(torch.log(torch.abs(module.weight / (module.running_var.sqrt() + module.eps))))\n",
    "    \n",
    "    # Account for dequantization scaling: [0,255] -> [0,1]\n",
    "    # Each pixel is scaled by 1/256, so density is scaled by 256\n",
    "    num_pixels = np.prod(batch.shape[1:])\n",
    "    dequant_log_det = num_pixels * np.log(256.0)\n",
    "    \n",
    "    # Total log likelihood\n",
    "    total_log_det = coupling_log_det + bn_log_det + dequant_log_det\n",
    "    log_likelihood = base_log_prob + total_log_det\n",
    "    \n",
    "    # L2 regularization on scale parameters (BJLKeng approach)\n",
    "    l2_loss = 0\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'weight' in name and 'coupling' in name:\n",
    "            l2_loss += torch.sum(param**2)\n",
    "    \n",
    "    # Negative log likelihood (our loss)\n",
    "    nll = -log_likelihood.mean() + l2_reg * l2_loss\n",
    "    \n",
    "    # Bits per dimension\n",
    "    bpd = nll / (np.log(2) * num_pixels)\n",
    "    \n",
    "    return nll, bpd, base_log_prob.mean(), coupling_log_det.mean(), bn_log_det, dequant_log_det\n",
    "\n",
    "# Test loss computation\n",
    "test_batch = trainX[:8].to(device)\n",
    "test_nll, test_bpd, test_base_lp, test_coupling_ld, test_bn_ld, test_dequant_ld = bjlkeng_loss_and_bpd(model, base_dist, test_batch)\n",
    "\n",
    "print(\"=== BJLKeng Loss Computation Test ===\")\n",
    "print(f\"NLL: {test_nll.item():.3f}\")\n",
    "print(f\"BPD: {test_bpd.item():.3f} (should be positive!)\")\n",
    "print(f\"Base log prob: {test_base_lp.item():.1f}\")\n",
    "print(f\"Coupling log det: {test_coupling_ld.item():.1f}\")\n",
    "print(f\"Batch norm log det: {test_bn_ld:.1f}\")\n",
    "print(f\"Dequantization log det: {test_dequant_ld:.1f}\")\n",
    "\n",
    "if test_bpd.item() > 0:\n",
    "    print(\"\\nðŸŽ‰ SUCCESS! BPD is positive with BJLKeng's approach!\")\nelse:\n",
    "    print(\"\\nðŸ˜ž Still negative - need more debugging\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BJLKeng's Training Approach\n",
    "\n",
    "Key training details:\n",
    "- **Small learning rate**: 0.0005 (\"slow learners\")\n",
    "- **Long training**: Many epochs\n",
    "- **L2 regularization**: Small weight decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_bjlkeng_realnvp(model, base_dist, dataloader, epochs=100, lr=0.0005):\n",
    "    \"\"\"BJLKeng's training approach\"\"\"\n",
    "    \n",
    "    # BJLKeng uses small learning rate\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-6)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=25, gamma=0.8)\n",
    "    \n",
    "    losses = []\n",
    "    bpds = []\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for epoch in tqdm(range(epochs), desc=\"Training BJLKeng Real NVP\"):\n",
    "        epoch_loss = 0\n",
    "        epoch_bpd = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        for batch_idx, batch in enumerate(dataloader):\n",
    "            if isinstance(batch, (list, tuple)):\n",
    "                batch = batch[0]\n",
    "            batch = batch.to(device)\n",
    "            \n",
    "            # BJLKeng's loss computation\n",
    "            nll, bpd, base_lp, coupling_ld, bn_ld, dequant_ld = bjlkeng_loss_and_bpd(model, base_dist, batch)\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            nll.backward()\n",
    "            \n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += nll.item()\n",
    "            epoch_bpd += bpd.item()\n",
    "            num_batches += 1\n",
    "            \n",
    "            # Print progress\n",
    "            if batch_idx % 300 == 0:\n",
    "                print(f\"Epoch {epoch+1:3d}, Batch {batch_idx:3d}: \"\n",
    "                      f\"NLL={nll.item():6.3f}, \"\n",
    "                      f\"BPD={bpd.item():5.3f}, \"\n",
    "                      f\"Base_LP={base_lp.item():7.1f}, \"\n",
    "                      f\"Coupling_LD={coupling_ld.item():6.1f}\")\n",
    "        \n",
    "        # Store epoch metrics\n",
    "        avg_loss = epoch_loss / num_batches\n",
    "        avg_bpd = epoch_bpd / num_batches\n",
    "        losses.append(avg_loss)\n",
    "        bpds.append(avg_bpd)\n",
    "        \n",
    "        scheduler.step()\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        \n",
    "        # Progress update\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"\\nEpoch {epoch+1}: BPD = {avg_bpd:.3f}, LR = {current_lr:.6f}\")\n",
    "            \n",
    "            # Generate samples\n",
    "            samples = model.sample(base_dist, n_samples=16)\n",
    "            samples = torch.clamp(samples, 0, 1)\n",
    "            \n",
    "            fig, axes = plt.subplots(4, 4, figsize=(8, 8))\n",
    "            for i in range(4):\n",
    "                for j in range(4):\n",
    "                    idx = i * 4 + j\n",
    "                    axes[i, j].imshow(samples[idx, 0].cpu(), cmap='gray')\n",
    "                    axes[i, j].set_xticks([])\n",
    "                    axes[i, j].set_yticks([])\n",
    "            \n",
    "            plt.suptitle(f'BJLKeng Real NVP Samples - Epoch {epoch+1} (BPD: {avg_bpd:.3f})')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "    \n",
    "    return losses, bpds\n",
    "\n",
    "# Setup training\n",
    "dataset = TensorDataset(trainX)\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=True, num_workers=0)\n",
    "\n",
    "print(f\"Ready to train with BJLKeng's approach\")\n",
    "print(f\"Target: ~1.9 bits/dim (BJLKeng's result)\")\n",
    "print(f\"Paper result: 1.06 bits/dim\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start BJLKeng Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_bpd.item() > 0:\n",
    "    print(\"ðŸš€ Starting BJLKeng-style training...\")\n",
    "    \n",
    "    bjlkeng_losses, bjlkeng_bpds = train_bjlkeng_realnvp(\n",
    "        model=model,\n",
    "        base_dist=base_dist,\n",
    "        dataloader=dataloader,\n",
    "        epochs=50,  # Start with fewer epochs for testing\n",
    "        lr=0.0005   # BJLKeng's learning rate\n",
    "    )\n",
    "    \n",
    "    # Plot results\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.plot(bjlkeng_losses)\n",
    "    plt.title('Training Loss (NLL)')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Negative Log Likelihood')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.plot(bjlkeng_bpds)\n",
    "    plt.title('Bits Per Dimension')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('BPD')\n",
    "    plt.axhline(y=1.92, color='g', linestyle='--', label='BJLKeng Result')\n",
    "    plt.axhline(y=1.06, color='r', linestyle='--', label='Paper Result')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.subplot(1, 3, 3)\n",
    "    # Show improvement over time\n",
    "    improvement = bjlkeng_bpds[0] - np.array(bjlkeng_bpds)\n",
    "    plt.plot(improvement)\n",
    "    plt.title('BPD Improvement')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('BPD Reduction from Start')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nðŸŽ¯ BJLKeng-Style Results:\")\n",
    "    print(f\"Initial BPD: {bjlkeng_bpds[0]:.3f}\")\n",
    "    print(f\"Final BPD: {bjlkeng_bpds[-1]:.3f}\")\n",
    "    print(f\"BJLKeng's result: 1.92\")\n",
    "    print(f\"Paper result: 1.06\")\n",
    "    print(f\"Total improvement: {bjlkeng_bpds[0] - bjlkeng_bpds[-1]:.3f} bits\")\n",
    "    \n",
    "    if bjlkeng_bpds[-1] < 3.0:\n",
    "        print(\"\\nðŸŽ‰ Success! Achieving reasonable BPD values!\")\n",
    "    \nelse:\n",
    "    print(\"âŒ BPD test failed - check loss computation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Sample Generation and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate final samples\n",
    "if 'bjlkeng_bpds' in locals():\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Generate larger batch of samples\n",
    "        final_samples = model.sample(base_dist, n_samples=100)\n",
    "        final_samples = torch.clamp(final_samples, 0, 1)\n",
    "    \n",
    "    # Show comparison with real data\n",
    "    fig, axes = plt.subplots(2, 10, figsize=(15, 4))\n",
    "    \n",
    "    # Real MNIST digits (top row)\n",
    "    for i in range(10):\n",
    "        axes[0, i].imshow(testX[i, 0], cmap='gray')\n",
    "        axes[0, i].set_title('Real' if i == 0 else '')\n",
    "        axes[0, i].set_xticks([])\n",
    "        axes[0, i].set_yticks([])\n",
    "    \n",
    "    # Generated samples (bottom row)\n",
    "    for i in range(10):\n",
    "        axes[1, i].imshow(final_samples[i, 0].cpu(), cmap='gray')\n",
    "        axes[1, i].set_title('Generated' if i == 0 else '')\n",
    "        axes[1, i].set_xticks([])\n",
    "        axes[1, i].set_yticks([])\n",
    "    \n",
    "    plt.suptitle(f'Real vs Generated MNIST (BJLKeng Real NVP - Final BPD: {bjlkeng_bpds[-1]:.3f})', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Large grid of generated samples\n",
    "    fig, axes = plt.subplots(10, 10, figsize=(12, 12))\n",
    "    for i in range(10):\n",
    "        for j in range(10):\n",
    "            idx = i * 10 + j\n",
    "            axes[i, j].imshow(final_samples[idx, 0].cpu(), cmap='gray')\n",
    "            axes[i, j].set_xticks([])\n",
    "            axes[i, j].set_yticks([])\n",
    "    \n",
    "    plt.suptitle(f'100 Generated MNIST Samples - BJLKeng Real NVP (BPD: {bjlkeng_bpds[-1]:.3f})', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: BJLKeng's Working Approach\n",
    "\n",
    "### âœ… **Key Insights from BJLKeng's Implementation:**\n",
    "\n",
    "1. **Simple Data Preprocessing**\n",
    "   - Just scale MNIST to [0,1] - no complex transforms\n",
    "   - Add dequantization noise, but keep it simple\n",
    "\n",
    "2. **Batch Normalization in Loss**\n",
    "   - Account for batch norm scaling in log-likelihood\n",
    "   - Use small momentum (0.05) for batch norm\n",
    "\n",
    "3. **Training Details**\n",
    "   - **Small learning rate**: 0.0005 (\"slow learners\")\n",
    "   - **L2 regularization**: Small weight decay on scale parameters\n",
    "   - **Long training**: Many epochs needed\n",
    "\n",
    "4. **Architecture**\n",
    "   - **Zero initialization**: Networks start at identity\n",
    "   - **Tanh scaling**: s = tanh(raw_s) for bounded scale\n",
    "   - **64 hidden channels**: Reasonable model size\n",
    "\n",
    "### ðŸ“Š **Expected Results:**\n",
    "- **BJLKeng achieved**: 1.92 bits/dim\n",
    "- **Original paper**: 1.06 bits/dim\n",
    "- **Good result**: < 3.0 bits/dim\n",
    "\n",
    "### ðŸŽ“ **Lessons Learned:**\n",
    "1. **Keep preprocessing simple** for MNIST\n",
    "2. **Account for ALL transformations** in loss (including batch norm)\n",
    "3. **Use small learning rates** - these models are slow learners\n",
    "4. **Zero initialization** is crucial for stable training\n",
    "\n",
    "This implementation follows BJLKeng's exact approach that achieved working results on MNIST! ðŸŽ‰"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}