{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Normalizing Flows\n",
    "\n",
    "Implementation based on https://github.com/AxelNathanson/pytorch-normalizing-flows/blob/main/flow_models.py\n",
    "\n",
    "This notebook implements convolutional Real NVP flows for image generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.distributions.multivariate_normal import MultivariateNormal\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Affine Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleAffine(nn.Module):\n",
    "    def __init__(self, mask, dim=784):\n",
    "        super().__init__()\n",
    "        self.mask = nn.Parameter(mask, requires_grad=False)\n",
    "        self.dim = dim\n",
    "        \n",
    "        self.s_func = nn.Sequential(\n",
    "            nn.Linear(dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "        self.t_func = nn.Sequential(\n",
    "            nn.Linear(dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x_masked = x * self.mask\n",
    "        s = self.s_func(x_masked)\n",
    "        t = self.t_func(x_masked)\n",
    "        \n",
    "        s = s * (1 - self.mask)\n",
    "        t = t * (1 - self.mask)\n",
    "        \n",
    "        y = x * torch.exp(s) + t\n",
    "        log_det_jac = s.sum(dim=1)\n",
    "        \n",
    "        return y, log_det_jac\n",
    "    \n",
    "    def inverse(self, y):\n",
    "        y_masked = y * self.mask\n",
    "        s = self.s_func(y_masked)\n",
    "        t = self.t_func(y_masked)\n",
    "        \n",
    "        s = s * (1 - self.mask)\n",
    "        t = t * (1 - self.mask)\n",
    "        \n",
    "        x = (y - t) * torch.exp(-s)\n",
    "        inv_log_det_jac = -s.sum(dim=1)\n",
    "        \n",
    "        return x, inv_log_det_jac"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stack Simple Affine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StackSimpleAffine(nn.Module):\n",
    "    def __init__(self, transforms, dim=784):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.transforms = nn.ModuleList(transforms)\n",
    "        self.distribution = MultivariateNormal(torch.zeros(self.dim), torch.eye(self.dim))\n",
    "\n",
    "    def log_probability(self, x):\n",
    "        log_prob = torch.zeros(x.shape[0], device=x.device)\n",
    "        for transform in reversed(self.transforms):\n",
    "            x, inv_log_det_jac = transform.inverse(x)\n",
    "            log_prob += inv_log_det_jac\n",
    "\n",
    "        log_prob += self.distribution.log_prob(x)\n",
    "        return log_prob\n",
    "\n",
    "    def rsample(self, num_samples):\n",
    "        x = self.distribution.sample((num_samples,))\n",
    "        log_prob = self.distribution.log_prob(x)\n",
    "\n",
    "        for transform in self.transforms:\n",
    "            x, log_det_jac = transform.forward(x)\n",
    "            log_prob += log_det_jac\n",
    "\n",
    "        return x, log_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real NVP Node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RealNVPNode(nn.Module):\n",
    "    def __init__(self, mask, dim=784):\n",
    "        super().__init__()\n",
    "        self.mask = nn.Parameter(mask, requires_grad=False)\n",
    "        self.dim = dim\n",
    "        \n",
    "        hidden_dim = 512\n",
    "        \n",
    "        self.s_func = nn.Sequential(\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "        self.t_func = nn.Sequential(\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x_masked = x * self.mask\n",
    "        s = self.s_func(x_masked)\n",
    "        t = self.t_func(x_masked)\n",
    "        \n",
    "        s = s * (1 - self.mask)\n",
    "        t = t * (1 - self.mask)\n",
    "        \n",
    "        y = x * torch.exp(s) + t\n",
    "        log_det_jac = s.sum(dim=1)\n",
    "        \n",
    "        return y, log_det_jac\n",
    "    \n",
    "    def inverse(self, y):\n",
    "        y_masked = y * self.mask\n",
    "        s = self.s_func(y_masked)\n",
    "        t = self.t_func(y_masked)\n",
    "        \n",
    "        s = s * (1 - self.mask)\n",
    "        t = t * (1 - self.mask)\n",
    "        \n",
    "        x = (y - t) * torch.exp(-s)\n",
    "        inv_log_det_jac = -s.sum(dim=1)\n",
    "        \n",
    "        return x, inv_log_det_jac"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real NVP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RealNVP(nn.Module):\n",
    "    def __init__(self, nodes, dim=784):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.nodes = nn.ModuleList(nodes)\n",
    "        self.distribution = MultivariateNormal(torch.zeros(self.dim), torch.eye(self.dim))\n",
    "\n",
    "    def log_probability(self, x):\n",
    "        log_prob = torch.zeros(x.shape[0], device=x.device)\n",
    "        for node in reversed(self.nodes):\n",
    "            x, inv_log_det_jac = node.inverse(x)\n",
    "            log_prob += inv_log_det_jac\n",
    "\n",
    "        log_prob += self.distribution.log_prob(x)\n",
    "        return log_prob\n",
    "\n",
    "    def rsample(self, num_samples):\n",
    "        x = self.distribution.sample((num_samples,))\n",
    "        log_prob = self.distribution.log_prob(x)\n",
    "\n",
    "        for node in self.nodes:\n",
    "            x, log_det_jac = node.forward(x)\n",
    "            log_prob += log_det_jac\n",
    "\n",
    "        return x, log_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Real NVP Node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RealNVPNodeCNN(nn.Module):\n",
    "    def __init__(self, mask, in_channels=1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.mask = nn.Parameter(mask, requires_grad=False)\n",
    "        \n",
    "        cnn_channels = [32, 64, 32]\n",
    "        \n",
    "        self.s_func = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, cnn_channels[0], kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(cnn_channels[0]),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(cnn_channels[0], cnn_channels[1], kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(cnn_channels[1]),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(cnn_channels[1], cnn_channels[2], kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(cnn_channels[2]),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(cnn_channels[2], in_channels, kernel_size=3, stride=1, padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "        self.t_func = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, cnn_channels[0], kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(cnn_channels[0]),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(cnn_channels[0], cnn_channels[1], kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(cnn_channels[1]),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(cnn_channels[1], cnn_channels[2], kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(cnn_channels[2]),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(cnn_channels[2], in_channels, kernel_size=3, stride=1, padding=1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x_masked = x * self.mask\n",
    "        s = self.s_func(x_masked)\n",
    "        t = self.t_func(x_masked)\n",
    "        \n",
    "        s = s * (1 - self.mask)\n",
    "        t = t * (1 - self.mask)\n",
    "        \n",
    "        y = x * torch.exp(s) + t\n",
    "        log_det_jac = s.sum(dim=[1, 2, 3])\n",
    "        \n",
    "        return y, log_det_jac\n",
    "    \n",
    "    def inverse(self, y):\n",
    "        y_masked = y * self.mask\n",
    "        s = self.s_func(y_masked)\n",
    "        t = self.t_func(y_masked)\n",
    "        \n",
    "        s = s * (1 - self.mask)\n",
    "        t = t * (1 - self.mask)\n",
    "        \n",
    "        x = (y - t) * torch.exp(-s)\n",
    "        inv_log_det_jac = -s.sum(dim=[1, 2, 3])\n",
    "        \n",
    "        return x, inv_log_det_jac"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Real NVP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RealNVPCNN(nn.Module):\n",
    "    def __init__(self, nodes, image_shape=(1, 28, 28)):\n",
    "        super().__init__()\n",
    "        self.image_shape = image_shape\n",
    "        self.dim = np.prod(image_shape)\n",
    "        self.nodes = nn.ModuleList(nodes)\n",
    "        self.distribution = MultivariateNormal(torch.zeros(self.dim), torch.eye(self.dim))\n",
    "\n",
    "    def log_probability(self, x):\n",
    "        log_prob = torch.zeros(x.shape[0], device=x.device)\n",
    "        for node in reversed(self.nodes):\n",
    "            x, inv_log_det_jac = node.inverse(x)\n",
    "            log_prob += inv_log_det_jac\n",
    "\n",
    "        x_flat = x.view(x.shape[0], -1)\n",
    "        log_prob += self.distribution.log_prob(x_flat)\n",
    "        return log_prob\n",
    "\n",
    "    def rsample(self, num_samples):\n",
    "        x = self.distribution.sample((num_samples,))\n",
    "        log_prob = self.distribution.log_prob(x)\n",
    "        \n",
    "        x = x.view(num_samples, *self.image_shape)\n",
    "\n",
    "        for node in self.nodes:\n",
    "            x, log_det_jac = node.forward(x)\n",
    "            log_prob += log_det_jac\n",
    "\n",
    "        return x, log_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST data\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Normalize to [-1, 1]\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
    "\n",
    "print(f'Training samples: {len(train_dataset)}')\n",
    "print(f'Test samples: {len(test_dataset)}')\n",
    "\n",
    "# Check data shape\n",
    "sample_batch = next(iter(train_loader))[0]\n",
    "print(f'Sample batch shape: {sample_batch.shape}')\n",
    "print(f'Data range: [{sample_batch.min():.3f}, {sample_batch.max():.3f}]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Checkerboard Masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_checkerboard_mask(height, width, reverse=False):\n",
    "    \"\"\"Create checkerboard mask for spatial coupling\"\"\"\n",
    "    mask = torch.zeros(height, width)\n",
    "    mask[::2, ::2] = 1  # Every other pixel starting from (0,0)\n",
    "    mask[1::2, 1::2] = 1  # Every other pixel starting from (1,1)\n",
    "    \n",
    "    if reverse:\n",
    "        mask = 1 - mask\n",
    "    \n",
    "    return mask\n",
    "\n",
    "# Create masks for CNN\n",
    "mask1 = create_checkerboard_mask(28, 28, reverse=False).unsqueeze(0).to(device)  # [1, 28, 28]\n",
    "mask2 = create_checkerboard_mask(28, 28, reverse=True).unsqueeze(0).to(device)   # [1, 28, 28]\n",
    "\n",
    "print(f'Mask 1 shape: {mask1.shape}')\n",
    "print(f'Mask 2 shape: {mask2.shape}')\n",
    "\n",
    "# Visualize masks\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "axes[0].imshow(mask1[0].cpu(), cmap='RdBu')\n",
    "axes[0].set_title('Mask 1')\n",
    "axes[0].set_xticks([])\n",
    "axes[0].set_yticks([])\n",
    "\n",
    "axes[1].imshow(mask2[0].cpu(), cmap='RdBu')\n",
    "axes[1].set_title('Mask 2')\n",
    "axes[1].set_xticks([])\n",
    "axes[1].set_yticks([])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Convolutional Real NVP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create CNN nodes with alternating masks\n",
    "num_layers = 6\n",
    "nodes = []\n",
    "\n",
    "for i in range(num_layers):\n",
    "    if i % 2 == 0:\n",
    "        nodes.append(RealNVPNodeCNN(mask1, in_channels=1))\n",
    "    else:\n",
    "        nodes.append(RealNVPNodeCNN(mask2, in_channels=1))\n",
    "\n",
    "# Create the model\n",
    "model = RealNVPCNN(nodes, image_shape=(1, 28, 28)).to(device)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'Total trainable parameters: {total_params:,}')\n",
    "\n",
    "# Test forward pass\n",
    "test_batch = next(iter(train_loader))[0][:4].to(device)\n",
    "with torch.no_grad():\n",
    "    log_prob = model.log_probability(test_batch)\n",
    "    samples, sample_log_prob = model.rsample(4)\n",
    "\n",
    "print(f'Test batch shape: {test_batch.shape}')\n",
    "print(f'Log probability shape: {log_prob.shape}')\n",
    "print(f'Sample shape: {samples.shape}')\n",
    "print(f'Sample log prob shape: {sample_log_prob.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, num_epochs=20, lr=1e-3):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.7)\n",
    "    \n",
    "    losses = []\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        progress_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}')\n",
    "        \n",
    "        for batch_idx, (data, _) in enumerate(progress_bar):\n",
    "            data = data.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Compute negative log likelihood\n",
    "            log_prob = model.log_probability(data)\n",
    "            loss = -log_prob.mean()\n",
    "            \n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            num_batches += 1\n",
    "            \n",
    "            # Update progress bar\n",
    "            progress_bar.set_postfix({\n",
    "                'Loss': f'{loss.item():.4f}',\n",
    "                'Avg Loss': f'{epoch_loss/num_batches:.4f}'\n",
    "            })\n",
    "        \n",
    "        avg_loss = epoch_loss / num_batches\n",
    "        losses.append(avg_loss)\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Average Loss: {avg_loss:.4f}')\n",
    "        \n",
    "        # Generate samples every 5 epochs\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            generate_samples(model, epoch + 1)\n",
    "    \n",
    "    return losses\n",
    "\n",
    "def generate_samples(model, epoch, num_samples=64):\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        samples, _ = model.rsample(num_samples)\n",
    "        samples = torch.clamp(samples, -1, 1)  # Clamp to [-1, 1]\n",
    "        samples = (samples + 1) / 2  # Convert to [0, 1] for visualization\n",
    "    \n",
    "    # Plot samples\n",
    "    fig, axes = plt.subplots(8, 8, figsize=(12, 12))\n",
    "    for i in range(8):\n",
    "        for j in range(8):\n",
    "            idx = i * 8 + j\n",
    "            axes[i, j].imshow(samples[idx, 0].cpu(), cmap='gray')\n",
    "            axes[i, j].axis('off')\n",
    "    \n",
    "    plt.suptitle(f'Generated Samples - Epoch {epoch}', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    model.train()\n",
    "\n",
    "# Start training\n",
    "print('Starting training...')\n",
    "losses = train_model(model, train_loader, num_epochs=20, lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Training Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(losses)\n",
    "plt.title('Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Negative Log Likelihood')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(f'Final loss: {losses[-1]:.4f}')\n",
    "print(f'Best loss: {min(losses):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Final Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a large batch of samples\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    final_samples, _ = model.rsample(100)\n",
    "    final_samples = torch.clamp(final_samples, -1, 1)\n",
    "    final_samples = (final_samples + 1) / 2\n",
    "\n",
    "# Plot comparison with real data\n",
    "real_data = next(iter(test_loader))[0][:10]\n",
    "real_data = (real_data + 1) / 2  # Convert to [0, 1]\n",
    "\n",
    "fig, axes = plt.subplots(2, 10, figsize=(15, 4))\n",
    "\n",
    "# Real data\n",
    "for i in range(10):\n",
    "    axes[0, i].imshow(real_data[i, 0], cmap='gray')\n",
    "    axes[0, i].axis('off')\n",
    "    if i == 0:\n",
    "        axes[0, i].set_title('Real Data', fontsize=12)\n",
    "\n",
    "# Generated data\n",
    "for i in range(10):\n",
    "    axes[1, i].imshow(final_samples[i, 0].cpu(), cmap='gray')\n",
    "    axes[1, i].axis('off')\n",
    "    if i == 0:\n",
    "        axes[1, i].set_title('Generated Data', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot large grid of generated samples\n",
    "fig, axes = plt.subplots(10, 10, figsize=(15, 15))\n",
    "for i in range(10):\n",
    "    for j in range(10):\n",
    "        idx = i * 10 + j\n",
    "        axes[i, j].imshow(final_samples[idx, 0].cpu(), cmap='gray')\n",
    "        axes[i, j].axis('off')\n",
    "\n",
    "plt.suptitle('100 Generated MNIST Samples', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    total_log_prob = 0\n",
    "    num_samples = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, _ in test_loader:\n",
    "            data = data.to(device)\n",
    "            log_prob = model.log_probability(data)\n",
    "            total_log_prob += log_prob.sum().item()\n",
    "            num_samples += data.size(0)\n",
    "    \n",
    "    avg_log_prob = total_log_prob / num_samples\n",
    "    bits_per_dim = -avg_log_prob / (np.log(2) * np.prod((1, 28, 28)))\n",
    "    \n",
    "    return avg_log_prob, bits_per_dim\n",
    "\n",
    "# Evaluate on test set\n",
    "test_log_prob, test_bpd = evaluate_model(model, test_loader)\n",
    "print(f'Test Log Probability: {test_log_prob:.4f}')\n",
    "print(f'Test Bits per Dimension: {test_bpd:.4f}')\n",
    "\n",
    "# Sample quality metrics\n",
    "with torch.no_grad():\n",
    "    samples, _ = model.rsample(1000)\n",
    "    sample_mean = samples.mean().item()\n",
    "    sample_std = samples.std().item()\n",
    "    \n",
    "print(f'\\nSample Statistics:')\n",
    "print(f'Sample Mean: {sample_mean:.4f}')\n",
    "print(f'Sample Std: {sample_std:.4f}')\n",
    "print(f'Sample Range: [{samples.min():.4f}, {samples.max():.4f}]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('=== Model Summary ===')\n",
    "print(f'Architecture: Convolutional Real NVP')\n",
    "print(f'Number of layers: {len(model.nodes)}')\n",
    "print(f'Total parameters: {total_params:,}')\n",
    "print(f'Image shape: {model.image_shape}')\n",
    "print(f'\\n=== Training Results ===')\n",
    "print(f'Final training loss: {losses[-1]:.4f}')\n",
    "print(f'Best training loss: {min(losses):.4f}')\n",
    "print(f'\\n=== Evaluation Results ===')\n",
    "print(f'Test log probability: {test_log_prob:.4f}')\n",
    "print(f'Test bits per dimension: {test_bpd:.4f}')\n",
    "print(f'\\nTraining completed successfully!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}